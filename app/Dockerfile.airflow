# Airflow Dockerfile for Stock Portfolio Pipeline
# Based on Apache Airflow 2.10.3

FROM apache/airflow:2.10.3-python3.11

USER root

# Install system dependencies and Docker CLI for spark-submit via docker exec
# Install OpenJDK 17 (available in Debian Bookworm) and other dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    libpq-dev \
    docker.io \
    curl \
    openjdk-17-jre-headless \
    procps \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME environment variable
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
# Add Java options for better compatibility with Spark (Java 17+)
ENV JAVA_OPTS="--add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED"
ENV PATH=$PATH:$JAVA_HOME/bin

# Add airflow user to docker group to allow docker commands
RUN usermod -aG docker airflow || true

# Copy and set up custom entrypoint
COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

USER airflow

# Copy requirements first for better layer caching
COPY requirements.airflow.txt .

# Install Python dependencies
RUN pip install -r requirements.airflow.txt

# Set working directory
WORKDIR /opt/airflow

# Use custom entrypoint
ENTRYPOINT ["/entrypoint.sh"]
CMD []
