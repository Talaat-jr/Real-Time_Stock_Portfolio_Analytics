{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5979457a",
   "metadata": {},
   "source": [
    "# Spark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc117424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, avg, count, desc, when\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8e43e",
   "metadata": {},
   "source": [
    "## spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b22943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session created successfully\n",
      "Spark Version: 3.5.3\n",
      "Master: spark://spark-master:7077\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StockPortfolioAnalysis\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"✅ Spark Session created successfully\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c617d",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9160ee40",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o25.read.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241m.\u001b[39mcsv(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFULL_STOCKS.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Adjust path as needed\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Data loaded successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal rows: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1706\u001b[0m, in \u001b[0;36mSparkSession.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrameReader:\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m    Returns a :class:`DataFrameReader` that can be used to read data\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    in as a :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;124;03m    +---+------------+\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameReader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:70\u001b[0m, in \u001b[0;36mDataFrameReader.__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, spark: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkSession\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m spark\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.read.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:92)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:75)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.DataFrameReader.<init>(DataFrameReader.scala:699)\n\tat org.apache.spark.sql.SparkSession.read(SparkSession.scala:783)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\n",
    "    \"FULL_STOCKS.csv\",  # Adjust path as needed\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Data loaded successfully\")\n",
    "print(f\"Total rows: {df.count()}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfaa81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FIRST 10 RECORDS\")\n",
    "print(\"=\"*80)\n",
    "df.show(10, truncate=False)\n",
    "\n",
    "print(\"\\nDataFrame Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3b011",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f5219",
   "metadata": {},
   "source": [
    "### Q1. Total Trading Volume by Stock Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Q1: Total Trading Volume for Each Stock Ticker\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "q1_result = df.groupBy(\"stock_ticker\") \\\n",
    "    .agg(spark_sum(\"quantity\").alias(\"total_volume\")) \\\n",
    "    .orderBy(desc(\"total_volume\"))\n",
    "\n",
    "q1_result.show(20, truncate=False)\n",
    "\n",
    "# Save results\n",
    "q1_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/q1_volume_by_ticker\")\n",
    "\n",
    "print(\"✅ Q1 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe836c",
   "metadata": {},
   "source": [
    "### Q2. verage Stock Price by Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Q2: Average Stock Price by Sector\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "q2_result = df.groupBy(\"stock_sector\") \\\n",
    "    .agg(avg(\"stock_price\").alias(\"average_price\")) \\\n",
    "    .orderBy(desc(\"average_price\"))\n",
    "\n",
    "q2_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "q2_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/q2_avg_price_by_sector\")\n",
    "\n",
    "print(\"✅ Q2 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df3276",
   "metadata": {},
   "source": [
    "### Q3. Weekend Transactions (Buy vs Sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Q3: Buy vs Sell Transactions on Weekends\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter for weekend transactions\n",
    "weekend_df = df.filter(col(\"is_weekend\") == 1)\n",
    "\n",
    "q3_result = weekend_df.groupBy(\"transaction_type\") \\\n",
    "    .agg(count(\"transaction_id\").alias(\"transaction_count\")) \\\n",
    "    .orderBy(desc(\"transaction_count\"))\n",
    "\n",
    "q3_result.show(truncate=False)\n",
    "\n",
    "total_weekend = weekend_df.count()\n",
    "print(f\"\\nTotal weekend transactions: {total_weekend}\")\n",
    "\n",
    "# Save results\n",
    "q3_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/q3_weekend_transactions\")\n",
    "\n",
    "print(\"✅ Q3 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484ebc5",
   "metadata": {},
   "source": [
    "### Q4. Customers with More Than 10 Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Q4: Customers with More Than 10 Transactions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "q4_result = df.groupBy(\"customer_id\") \\\n",
    "    .agg(count(\"transaction_id\").alias(\"transaction_count\")) \\\n",
    "    .filter(col(\"transaction_count\") > 10) \\\n",
    "    .orderBy(desc(\"transaction_count\"))\n",
    "\n",
    "print(f\"Total customers with >10 transactions: {q4_result.count()}\")\n",
    "print(\"\\nTop 20 customers:\")\n",
    "q4_result.show(20, truncate=False)\n",
    "\n",
    "# Save results\n",
    "q4_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/q4_active_customers\")\n",
    "\n",
    "print(\"✅ Q4 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5af84b",
   "metadata": {},
   "source": [
    "### Q5. Total Trade Amount by Day of Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Q5: Total Trade Amount per Day of Week (Highest to Lowest)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "q5_result = df.groupBy(\"day_name\") \\\n",
    "    .agg(spark_sum(\"total_trade_amount\").alias(\"total_trade_amount\")) \\\n",
    "    .orderBy(desc(\"total_trade_amount\"))\n",
    "\n",
    "q5_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "q5_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/q5_trade_by_day\")\n",
    "\n",
    "print(\"✅ Q5 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc6c538",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Register DataFrame for SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"stocks\")\n",
    "print(\"✅ DataFrame registered as 'stocks' view for SQL queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa15b12b",
   "metadata": {},
   "source": [
    "### Q1. Top 5 Most Traded Stocks by Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2510dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SQL Q1: Top 5 Most Traded Stock Tickers by Total Quantity\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sql_query_1 = \"\"\"\n",
    "    SELECT \n",
    "        stock_ticker,\n",
    "        SUM(quantity) as total_quantity\n",
    "    FROM stocks\n",
    "    GROUP BY stock_ticker\n",
    "    ORDER BY total_quantity DESC\n",
    "    LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "sql_q1_result = spark.sql(sql_query_1)\n",
    "sql_q1_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "sql_q1_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/sql_q1_top_5_stocks\")\n",
    "\n",
    "print(\"✅ SQL Q1 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cf510b",
   "metadata": {},
   "source": [
    "### Q2. Average Trade Amount by Account Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4f65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SQL Q2: Average Trade Amount by Customer Account Type\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sql_query_2 = \"\"\"\n",
    "    SELECT \n",
    "        customer_account_type,\n",
    "        AVG(total_trade_amount) as avg_trade_amount,\n",
    "        COUNT(*) as transaction_count\n",
    "    FROM stocks\n",
    "    GROUP BY customer_account_type\n",
    "    ORDER BY avg_trade_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "sql_q2_result = spark.sql(sql_query_2)\n",
    "sql_q2_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "sql_q2_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/sql_q2_avg_by_account\")\n",
    "\n",
    "print(\"✅ SQL Q2 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9684a5",
   "metadata": {},
   "source": [
    "### Q3. Holiday vs Non-Holiday Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SQL Q3: Transactions During Holidays vs Non-Holidays\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sql_query_3 = \"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN is_holiday = 1 THEN 'Holiday'\n",
    "            ELSE 'Non-Holiday'\n",
    "        END as period_type,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(total_trade_amount) as total_trade_amount\n",
    "    FROM stocks\n",
    "    GROUP BY is_holiday\n",
    "    ORDER BY is_holiday DESC\n",
    "\"\"\"\n",
    "\n",
    "sql_q3_result = spark.sql(sql_query_3)\n",
    "sql_q3_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "sql_q3_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/sql_q3_holiday_comparison\")\n",
    "\n",
    "print(\"✅ SQL Q3 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6160bc85",
   "metadata": {},
   "source": [
    "### Q4. Sectors with Highest Weekend Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c45bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SQL Q4: Stock Sectors with Highest Weekend Trading Volume\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sql_query_4 = \"\"\"\n",
    "    SELECT \n",
    "        stock_sector,\n",
    "        SUM(quantity) as total_weekend_volume,\n",
    "        COUNT(*) as weekend_transactions,\n",
    "        SUM(total_trade_amount) as total_weekend_value\n",
    "    FROM stocks\n",
    "    WHERE is_weekend = 1\n",
    "    GROUP BY stock_sector\n",
    "    ORDER BY total_weekend_volume DESC\n",
    "\"\"\"\n",
    "\n",
    "sql_q4_result = spark.sql(sql_query_4)\n",
    "sql_q4_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "sql_q4_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/sql_q4_weekend_sectors\")\n",
    "\n",
    "print(\"✅ SQL Q4 completed and saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d100ae61",
   "metadata": {},
   "source": [
    "### Q5. Buy vs Sell by Liquidity Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4541d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SQL Q5: Total Buy vs Sell Amount by Stock Liquidity Tier\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sql_query_5 = \"\"\"\n",
    "    SELECT \n",
    "        stock_liquidity_tier,\n",
    "        SUM(CASE WHEN transaction_type = 'BUY' THEN total_trade_amount ELSE 0 END) as total_buy_amount,\n",
    "        SUM(CASE WHEN transaction_type = 'SELL' THEN total_trade_amount ELSE 0 END) as total_sell_amount,\n",
    "        SUM(total_trade_amount) as total_amount,\n",
    "        COUNT(CASE WHEN transaction_type = 'BUY' THEN 1 END) as buy_count,\n",
    "        COUNT(CASE WHEN transaction_type = 'SELL' THEN 1 END) as sell_count\n",
    "    FROM stocks\n",
    "    GROUP BY stock_liquidity_tier\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\"\n",
    "\n",
    "sql_q5_result = spark.sql(sql_query_5)\n",
    "sql_q5_result.show(truncate=False)\n",
    "\n",
    "# Save results\n",
    "sql_q5_result.coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"output/spark_results/sql_q5_liquidity_analysis\")\n",
    "\n",
    "print(\"✅ SQL Q5 completed and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95a6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"✅ All 5 Spark DataFrame questions completed\")\n",
    "print(\"✅ All 5 Spark SQL questions completed\")\n",
    "print(\"✅ Results saved to output/spark_results/\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\n✅ Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
